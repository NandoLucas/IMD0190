{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAI8aPsIJwti"
      },
      "source": [
        "## IMD 0190 - TÓPICOS ESPECIAIS EM BUSINESS INTELIGENCE E ANALYTICS\n",
        "\n",
        "#### Fernando Lucas, Renata Gurgel e Victor Gabriel\n",
        "\n",
        "### Sumário\n",
        "#### Principais pontos\n",
        "\n",
        "- O portal de dados abertos expõe dados de transparência do TRE/RN agrupados em grandes áreas, tais como: Secretaria de Gestão de Pessoas; Secretaria de Tecnologia da Informação e Eleições; Zonas Eleitorais.\n",
        "\n",
        "- O portal permite que os dados sejam baixados no formato csv sem necessidade de solicitação, basta acessar o site e selecionar o assunto de interesse. O site está disponível em: https://dados.tre-rn.jus.br/.\n",
        "\n",
        "- O projeto sucintamente consiste na implementação de um agente de inteligência artificial para solucionar dúvidas e realizar análises iniciais sobre os dataframes.\n",
        "\n",
        "- Utilização da API gratuita do *GEMINI* e *Ollama* como LLMs para desenvolvimento do projeto.\n",
        "\n",
        "### Resumo\n",
        "\n",
        "- Desenvolvimento de um Agente de IA que seja capaz de responder as questões dos usuários referente aos dados disponíveis no Portal de dados abertos do Tribunal Regional Eleitoral do Rio Grande do Norte (TRE/RN). O portal referido acima pode ser acessado através do link: https://dados.tre-rn.jus.br/group/pessoas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GthkaiKwUh0Z"
      },
      "source": [
        "### Importar bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVEjOBiIGXET",
        "outputId": "c919d556-0095-4aaf-d2dc-60824e02abfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.28 (from langchain_experimental)\n",
            "  Downloading langchain_core-0.3.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.11-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.38-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain)\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.11.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting numpy<3,>=1.26.2 (from langchain)\n",
            "  Downloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests)\n",
            "  Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Downloading google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Downloading google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Downloading google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting google-api-core (from google-generativeai)\n",
            "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting google-api-python-client (from google-generativeai)\n",
            "  Downloading google_api_python_client-2.162.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
            "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting tqdm (from google-generativeai)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from google-generativeai)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading proto_plus-1.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpx<0.29,>=0.27 (from ollama)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
            "  Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting anyio (from httpx<0.29,>=0.27->ollama)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting httpcore==1.* (from httpx<0.29,>=0.27->ollama)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.29,>=0.27->ollama)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.28->langchain_experimental)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
            "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
            "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
            "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai)\n",
            "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx<0.29,>=0.27->ollama)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
            "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
            "Downloading aiohttp-3.11.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m485.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.8/210.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m781.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.40-py3-none-any.whl (414 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading langsmith-0.3.11-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.3/335.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.38-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_api_python_client-2.162.0-py2.py3-none-any.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m767.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.6/283.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading proto_plus-1.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.9/336.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.70.0-py3-none-any.whl (14 kB)\n",
            "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m877.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m994.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: filetype, zstandard, urllib3, uritemplate, typing-extensions, tqdm, tenacity, sniffio, PyYAML, python-dotenv, pyparsing, pyasn1, protobuf, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, httpx-sse, h11, grpcio, greenlet, frozenlist, charset-normalizer, certifi, cachetools, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, rsa, requests, pydantic-core, pyasn1-modules, proto-plus, jsonpatch, httplib2, httpcore, googleapis-common-protos, anyio, aiosignal, requests-toolbelt, pydantic, httpx, grpcio-status, google-auth, dataclasses-json, aiohttp, pydantic-settings, ollama, langsmith, google-auth-httplib2, google-api-core, langchain-core, google-api-python-client, langchain-text-splitters, langchain_ollama, google-ai-generativelanguage, langchain, google-generativeai, langchain_google_genai, langchain-community, langchain_experimental\n",
            "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.38 aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 dataclasses-json-0.6.7 filetype-1.2.0 frozenlist-1.5.0 google-ai-generativelanguage-0.6.15 google-api-core-2.24.1 google-api-python-client-2.162.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 googleapis-common-protos-1.68.0 greenlet-3.1.1 grpcio-1.70.0 grpcio-status-1.70.0 h11-0.14.0 httpcore-1.0.7 httplib2-0.22.0 httpx-0.28.1 httpx-sse-0.4.0 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.40 langchain-text-splitters-0.3.6 langchain_experimental-0.3.4 langchain_google_genai-2.0.10 langchain_ollama-0.2.3 langsmith-0.3.11 marshmallow-3.26.1 multidict-6.1.0 mypy-extensions-1.0.0 numpy-2.2.3 ollama-0.4.7 orjson-3.10.15 propcache-0.3.0 proto-plus-1.26.0 protobuf-5.29.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.6 pydantic-core-2.27.2 pydantic-settings-2.8.1 pyparsing-3.2.1 python-dotenv-1.0.1 requests-2.32.3 requests-toolbelt-1.0.0 rsa-4.9 sniffio-1.3.1 tenacity-9.0.0 tqdm-4.67.1 typing-extensions-4.12.2 typing-inspect-0.9.0 uritemplate-4.1.1 urllib3-2.3.0 yarl-1.18.3 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_experimental langchain requests langchain-community langchain_google_genai google-generativeai langchain_ollama ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Text Classification - An Real Application to Escola Judiciária Eleitoral do Rio Grande do Norte (EJE)\n",
        "\n",
        "- This is a init for a reviews courses classifications using the method of Zero-Shot Text Classification\n",
        "\n",
        "- We were based of: https://github.com/eliasjacob/deep_learning_gen_ai/blob/main/Notebook_10.ipynb\n",
        "\n",
        "- We was incrementing the project step-by-step. So, some codes can be repetitive with different approaches\n",
        "\n",
        "- The first implementation we have defined the schema using reviews segmented in *Positive*, *Negative* and *Neutral* generated by `DeepSeek-V3`. Then, use the `.with_structured_method` to format the output response about the sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openpyxl pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Defining the LLM models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/disciplinas/gen-ai/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import ChatOllama class from langchain_ollama module\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "gemini_api_key = \"\"\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=gemini_api_key,\n",
        "    temperature=0.0\n",
        "    )\n",
        "\n",
        "# Initialize the ChatOllama model with specific parameters\n",
        "model_llama = ChatOllama(\n",
        "    model=\"llama3.1\",  # Specify the model to use\n",
        "    base_url=\"http://localhost:11434\",  # Set the base URL where the Ollama service is running\n",
        "    temperature=0.0,  # Set the temperature for response variability\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lists of reviews\n",
        "\n",
        "- We use *DeepSeek-V3* to generate feedbacks lists for each of the possible classifications: positive, negative and neutral;\n",
        "\n",
        "- Then, for test purposes *unlabelled feedbacks* was set as the all reviews in each lists;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a example list of negative feedbacks\n",
        "negative_feedbacks = [\n",
        "    \"O conteúdo é muito superficial, não aprofunda os temas.\",\n",
        "    \"As aulas são longas e cansativas, difícil manter a atenção.\",\n",
        "    \"Faltou material prático para aplicar o que foi ensinado.\",\n",
        "    \"A plataforma é lenta e trava constantemente.\",\n",
        "    \"O instrutor não explica de forma clara, fica confuso.\",\n",
        "    \"O curso é caro para o pouco conteúdo que oferece.\",\n",
        "    \"Não gostei da falta de interação com os instrutores.\",\n",
        "    \"O certificado não é reconhecido no mercado.\",\n",
        "    \"As atividades são repetitivas e pouco desafiadoras.\",\n",
        "    \"O suporte é demorado e não resolve os problemas direito.\"\n",
        "]\n",
        "# This is a example list of positive feedbacks\n",
        "positive_feedbacks = [\n",
        "    \"O curso é excelente, conteúdo muito bem organizado!\",\n",
        "    \"Aprendi muito e consegui aplicar no meu trabalho. Recomendo!\",\n",
        "    \"Instrutores claros e didáticos. Foi uma ótima experiência.\",\n",
        "    \"O material de apoio é completo e fácil de entender.\",\n",
        "    \"Adorei a plataforma, muito intuitiva e fácil de usar.\",\n",
        "    \"O curso superou minhas expectativas. Parabéns à equipe!\",\n",
        "    \"Conteúdo atualizado e relevante para o mercado atual.\",\n",
        "    \"As atividades práticas ajudaram a fixar o conhecimento.\",\n",
        "    \"Suporte rápido e eficiente. Tive todas as minhas dúvidas resolvidas.\",\n",
        "    \"A interação com os alunos foi excelente.\"\n",
        "]\n",
        "\n",
        "# This is a example list of neutral feedbacks\n",
        "neutral_feedbacks = [\n",
        "    \"O curso é bom, mas poderia ter mais exemplos práticos.\",\n",
        "    \"O conteúdo é interessante, mas algumas aulas são muito longas.\",\n",
        "    \"Gostei do material, mas a plataforma poderia ser mais estável.\",\n",
        "    \"O curso atendeu às minhas expectativas, mas nada extraordinário.\",\n",
        "    \"As explicações são claras, mas o ritmo é um pouco lento.\",\n",
        "    \"O conteúdo é útil, mas senti falta de mais interação com os instrutores.\",\n",
        "    \"O curso é razoável, mas o preço poderia ser mais acessível.\",\n",
        "    \"Achei o material completo, mas algumas partes são repetitivas.\",\n",
        "    \"O curso é bom para iniciantes, mas avançados podem achar básico.\",\n",
        "    \"A experiência foi ok, mas esperava mais atividades práticas.\"\n",
        "]\n",
        "\n",
        "# This is a example list of feedbacks not classified\n",
        "unlabelled_feedbacks = positive_feedbacks + negative_feedbacks + neutral_feedbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method Implementation: '.with_structured_output()'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "class SentimentAnalysisResponse(BaseModel):\n",
        "    \"\"\"The response of a function that performs sentiment analysis on text.\"\"\"\n",
        "\n",
        "    # The sentiment label assigned to the text\n",
        "    sentiment: Literal[\"positive\", \"neutral\", \"negative\"] = Field(\n",
        "        default_factory=str,\n",
        "        description=\"The sentiment label assigned to the text. You can only have 'positive' or 'negative' as values.\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prompting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SentimentAnalysisResponse(sentiment='negative')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_sentiment_gemini = gemini.with_structured_output(SentimentAnalysisResponse)\n",
        "\n",
        "output = model_sentiment_gemini.invoke(\"A aula não foi muito boa e as atividades não foram interessantes como o esperado.\")\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Conclusion: quickly and with little development effort, it is possible to obtain an application that solves a real problem for people who really need it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementing the Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Inicialmente criamos duas ferramentas, uma responsável por responder a avaliação analisada e a classificação, e a outra responsável por fazer a contabilização total de cada classe.\n",
        "\n",
        "- Em contato com o cliente, percebemos que faz mais sentido a centralização em uma única ferramenta capaz de responder cada classificação por avaliação caso receba uma lista de comentários e a contabilização total, e caso receba apenas uma, responsa a classificação apenas.\n",
        "\n",
        "- Os nomes delas são: `analyze_sentiment()` e `count_sentiments()`.\n",
        "\n",
        "- Nós utilizamos a função do *LangChain* `create_tool_calling_agent` para criação do agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_76519/1241167377.py:59: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Literal, Union\n",
        "from langchain.tools import tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from collections import Counter\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "class SentimentAnalysisResponse(BaseModel):\n",
        "    \"\"\"The response of a function that performs sentiment analysis on text.\"\"\"\n",
        "\n",
        "    # The sentiment label assigned to the text\n",
        "    sentiment: Literal[\"positive\", \"neutral\", \"negative\"] = Field(\n",
        "        default_factory=str,\n",
        "        description=\"The sentiment label assigned to the text. You can only have 'positive', 'neutral' or 'negative' as values.\",\n",
        "    )\n",
        "model_sentiment_gemini = gemini.with_structured_output(SentimentAnalysisResponse)\n",
        "# Criar um Tool para análise de sentimentos\n",
        "@tool\n",
        "def analyze_sentiment(texts: Union[str, List[str]]) -> dict:\n",
        "    \"\"\"\n",
        "    Recebe um único texto ou uma lista de textos e retorna a classificação de sentimento de cada um.\n",
        "    Se for uma lista, também retorna a contagem total de cada classificação.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(texts, str):\n",
        "        # Caso seja um único texto, retorna apenas sua classificação\n",
        "        return model_sentiment_gemini.invoke(texts)\n",
        "\n",
        "    elif isinstance(texts, list):\n",
        "        # Caso seja uma lista, processa cada um e gera um resumo da contagem\n",
        "        sentiment_counts = Counter()\n",
        "        individual_results = []\n",
        "\n",
        "        for text in texts:\n",
        "            result = model_sentiment_gemini.invoke(text)\n",
        "            sentiment_counts[result.sentiment] += 1\n",
        "            individual_results.append(result)\n",
        "\n",
        "        return {\n",
        "            \"individual_results\": individual_results,\n",
        "            \"total_counts\": dict(sentiment_counts)\n",
        "        }\n",
        "\n",
        "\n",
        "# Criar um Tool para contar os sentimentos\n",
        "@tool\n",
        "def count_sentiments(texts: List[str]) -> dict:\n",
        "    \"\"\"Recebe uma lista de textos e retorna a contagem de sentimentos (positivo, neutro, negativo).\"\"\"\n",
        "    sentiment_counts = Counter()\n",
        "\n",
        "    for text in texts:\n",
        "        output = model_sentiment_gemini.invoke(text)\n",
        "        sentiment_counts[output.sentiment] += 1\n",
        "\n",
        "    return dict(sentiment_counts)\n",
        "\n",
        "# Criar um agente com memória\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Define a prompt template for the chat, including system instructions and placeholders\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")\n",
        "tools=[analyze_sentiment]\n",
        "# Inicializar o agente\n",
        "agent = create_tool_calling_agent(\n",
        "    tools=tools,  # Tools disponíveis para o agente\n",
        "    llm=gemini,\n",
        "    prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- A fonte de dados é proveniente do google forms e após todos os alunos responderem, os dados podem ser baixados em diversos formatos, dentre eles: **csv**.\n",
        "\n",
        "- Dessa forma, importamos o dataframe e convertemos para uma lista de com as avaliações não nulas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Configuração da fonte de dados\n",
        "sheet_id = \"\"\n",
        "sheet_name = \"\"  # Certifique-se de usar o nome correto da aba\n",
        "\n",
        "# URL gerada pelo Google Sheets para exportação como CSV\n",
        "url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "\n",
        "# Carregar os dados no Pandas\n",
        "df_drive = pd.read_csv(url, header=None, usecols=[14], dtype=str)\n",
        "df_manual = pd.read_csv(\"./Avaliação de Reação.csv\", usecols=[14])\n",
        "\n",
        "reviews = df_manual.dropna()\n",
        "reviews.head(50)\n",
        "reviews_filtered = reviews.head()\n",
        "unlabelled_feedbacks = reviews_filtered['Unnamed: 14'].tolist()\n",
        "print(len(unlabelled_feedbacks))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Após definição do agente, podemos utilizar o `AgentExecutor()` também do *LangChain* para executar o agente recebendo uma entrada do usuário.\n",
        "\n",
        "- Utilizamos 5 dados não classificados previamente obtidos pelo dataframe inserido anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `analyze_sentiment` with `{'texts': ['Tudo ótimo.', 'Ótimo curso.', 'Treinamento oportuno', 'Nada a acrescentar', 'Curso ótimo!']}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'individual_results': [SentimentAnalysisResponse(sentiment='positive'), SentimentAnalysisResponse(sentiment='positive'), SentimentAnalysisResponse(sentiment='positive'), SentimentAnalysisResponse(sentiment=''), SentimentAnalysisResponse(sentiment='positive')], 'total_counts': {'positive': 4, '': 1}}\u001b[0m\u001b[32;1m\u001b[1;3mA classificação de sentimento para os textos fornecidos é:\n",
            "- Tudo ótimo.: positivo\n",
            "- Ótimo curso.: positivo\n",
            "- Treinamento oportuno: positivo\n",
            "- Nada a acrescentar: sem sentimento\n",
            "- Curso ótimo!: positivo\n",
            "\n",
            "Além disso, a contagem total de cada classificação é:\n",
            "- positivo: 4\n",
            "- sem sentimento: 1\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"Informe a classificação para cada um dos textos em ['Tudo ótimo.', 'Ótimo curso.', 'Treinamento oportuno', 'Nada a acrescentar', 'Curso ótimo!']\",\n",
              " 'output': 'A classificação de sentimento para os textos fornecidos é:\\n- Tudo ótimo.: positivo\\n- Ótimo curso.: positivo\\n- Treinamento oportuno: positivo\\n- Nada a acrescentar: sem sentimento\\n- Curso ótimo!: positivo\\n\\nAlém disso, a contagem total de cada classificação é:\\n- positivo: 4\\n- sem sentimento: 1'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of AgentExecutor with specified agent, tools, and verbosity\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# Invoke the agent_executor with a dictionary containing the input query\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": f\"Informe a classificação para cada um dos textos em {unlabelled_feedbacks}\",  # The input query asking for the price of Bitcoin\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Criamos uma nova ferramenta capaz de receber o dataframe, fazer o tratamento para converter em lista e fazer a classificação e contabilização de cada classe.\n",
        "\n",
        "- Como o setor que utilizará a ferramenta tem um padrao na fonte de dados em que a coluna que interessa é sempre a mesma e, devido a ela não ser nomeada (padrão pós google forms), utilizamos a ferramenta para receber apenas o dataframe sem identificação da coluna alvo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def process_dataframe_for_sentiment(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Recebe um DataFrame e sempre usa a coluna de índice 14 para análise de sentimentos.\n",
        "    Retorna os resultados individuais e a contagem total de classificações.\n",
        "    \"\"\"\n",
        "\n",
        "    # Verifica se o índice 14 está dentro do DataFrame\n",
        "    if len(df.columns) <= 14:\n",
        "        return {\"error\": f\"O DataFrame possui apenas {len(df.columns)} colunas. A coluna de índice 14 não existe.\"}\n",
        "\n",
        "    column = df.columns[14]  # Obtém o nome da coluna de índice 14\n",
        "    texts = df[column].dropna().tolist()\n",
        "\n",
        "    if not texts:\n",
        "        return {\"error\": \"A coluna 14 não contém valores válidos para análise.\"}\n",
        "\n",
        "    # Chamar analyze_sentiment para cada texto\n",
        "    sentiment_counts = Counter()\n",
        "    individual_results = []\n",
        "\n",
        "    for text in texts:\n",
        "        result = model_sentiment_gemini.invoke(text)\n",
        "        sentiment_counts[result.sentiment] += 1\n",
        "        individual_results.append(result)\n",
        "\n",
        "    return {\n",
        "        \"individual_results\": individual_results,  # Lista de SentimentAnalysisResponse\n",
        "        \"total_counts\": dict(sentiment_counts)  # Contagem dos sentimentos\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Implementação da interface no streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "# 🔹 Interface do Streamlit\n",
        "st.title(\"Análise de Sentimentos com LLM 🚀\")\n",
        "st.write(\"Digite um texto, uma lista de textos ou faça upload de um CSV.\")\n",
        "\n",
        "# 📌 **Entrada de texto do usuário**\n",
        "user_input = st.text_area(\"Digite um comando para o agente:\")\n",
        "\n",
        "# 📌 **Upload do CSV**\n",
        "uploaded_file = st.file_uploader(\"Ou carregue um arquivo CSV\", type=[\"csv\"])\n",
        "\n",
        "df = None\n",
        "if uploaded_file:\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "    st.subheader(\"📊 Prévia do DataFrame\")\n",
        "    st.write(df.head())\n",
        "\n",
        "if st.button(\"Executar Análise\"):\n",
        "    if user_input:\n",
        "        with st.spinner(\"O agente está processando sua solicitação...\"):\n",
        "            result = agent_executor.invoke({\"input\": user_input})\n",
        "    elif df is not None:\n",
        "        with st.spinner(\"O agente está analisando os dados...\"):\n",
        "            result = agent_executor.invoke({\"input\": \"Analise o sentimento do DataFrame.\"})\n",
        "\n",
        "    if \"error\" in result:\n",
        "        st.error(result[\"error\"])\n",
        "    else:\n",
        "        if \"individual_results\" in result:\n",
        "            st.subheader(\"📌 Classificação Individual\")\n",
        "            st.write(pd.DataFrame(result[\"individual_results\"]))\n",
        "\n",
        "        if \"total_counts\" in result:\n",
        "            st.subheader(\"📊 Resumo da Análise\")\n",
        "            st.write(result[\"total_counts\"])\n",
        "\n",
        "            # Criando o gráfico de barras\n",
        "            st.subheader(\"📊 Distribuição dos Sentimentos\")\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.bar(result[\"total_counts\"].keys(), result[\"total_counts\"].values(), color=[\"green\", \"gray\", \"red\"])\n",
        "            ax.set_xlabel(\"Sentimentos\")\n",
        "            ax.set_ylabel(\"Quantidade\")\n",
        "            ax.set_title(\"Distribuição dos Sentimentos no Dataset\")\n",
        "            st.pyplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhQgIRVOShf7"
      },
      "source": [
        "### Pandas Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "a85Lv2zUJS4H",
        "outputId": "24f48d68-68b4-412d-b870-54d7dbf91e96"
      },
      "outputs": [],
      "source": [
        "# Import the function to create an agent that works with pandas DataFrames\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "# Import the pandas library for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Servidor_Ativo dataset from the provided URL into a pandas DataFrame\n",
        "#df = pd.read_csv(\"/content/servidor_ativo.csv\", delimiter=\";\")\n",
        "# Read the Ex_Servidor dataset from the provided URL into a pandas DataFrame\n",
        "#df1 = pd.read_csv(\"/content/ex_servidor.csv\", delimiter=\";\")\n",
        "# Read the Servidor_Inativo dataset from the provided URL into a pandas DataFrame\n",
        "#df2 = pd.read_csv(\"/content/servidor_inativo.csv\", delimiter=\";\")\n",
        "# Read the Servidor_Ausente dataset from the provided URL into a pandas DataFrame\n",
        "#df3 = pd.read_csv(\"/content/servidor_ausente.csv\", delimiter=\";\")\n",
        "# Read the Servidor_Instituidor_Pensao dataset from the provided URL into a pandas DataFrame\n",
        "#df4 = pd.read_csv(\"/content/servidor_instituidor_pensao.csv\", delimiter=\";\")\n",
        "df_diarias = pd.read_csv(\"./diarias-historico.csv\", delimiter=\";\")\n",
        "df_passagens = pd.read_csv(\"./passagens-historico.csv\", delimiter=\";\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the data has been loaded correctly\n",
        "df_diarias.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dHU7FW4xrHzI",
        "outputId": "22b90b91-4d0e-48a9-f8a5-5b35ba102df5"
      },
      "outputs": [],
      "source": [
        "df_passagens.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oofh34iNwI0m",
        "outputId": "6ba5800d-9113-47a2-ca01-ce96883d505b"
      },
      "outputs": [],
      "source": [
        "df_passagens['COMPANHIA'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhHp4EpzHOp8"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import os\n",
        "\n",
        "gemini_api_key = \"\"\n",
        "gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=gemini_api_key,\n",
        "    temperature=0.0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1izsTEmDYGIR"
      },
      "outputs": [],
      "source": [
        "# Criar um PromptTemplate para reformular as respostas de forma mais conversacional\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"\"\"\n",
        "    Você é um assistente amigável e prestativo. Sua tarefa é reformular a resposta fornecida de maneira clara, explicativa e simpática. \n",
        "    A resposta deve ser dada em Português-Brasil e deve soar natural, como se fosse uma conversa.\n",
        "\n",
        "    Resposta original: {query}\n",
        "\n",
        "    Resposta reformulada:\n",
        "    \"\"\"\n",
        ")\n",
        "pipeline = prompt_template | gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GowGqkpPT4D"
      },
      "outputs": [],
      "source": [
        "# Adicionar um prefixo ao prompt para explicar os dataframes\n",
        "prefixo = \"\"\"\n",
        "Você tem acesso a dois dataframes:\n",
        "1. `df_diarias`: Contém informações sobre diárias pagas.\n",
        "2. `df_passagens`: Contém informações sobre passagens aéreas.\n",
        "\n",
        "Use o dataframe apropriado para responder às perguntas.\n",
        "\"\"\"\n",
        "# Create a pandas dataframe agent using the specified LLM model and dataframe\n",
        "pandas_agent = create_pandas_dataframe_agent(\n",
        "    llm=gemini,  # Pass the initialized ChatOpenAI model\n",
        "    df=[df_diarias, df_passagens],  # Provide the dataframe to be used by the agent\n",
        "    verbose=True,  # Enable verbose mode for detailed logging\n",
        "    agent_type=\"zero-shot-react-description\",  # Specify the type of agent to create\n",
        "    allow_dangerous_code=True,  # Opt-in to allow the use of the REPL tool\n",
        "    prefix=prefixo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0g3fHn9ZCbc"
      },
      "outputs": [],
      "source": [
        "def humanized_agent_response(query):\n",
        "    try:\n",
        "        # Obter a resposta original do pandas_agent\n",
        "        resposta_original = pandas_agent.invoke(query)\n",
        "\n",
        "        # Garantir que a resposta seja uma string\n",
        "        if isinstance(resposta_original, dict):\n",
        "            resposta_original = resposta_original.get(\"output\", \"\") or resposta_original.get(\"input\", \"\")\n",
        "        resposta_original = str(resposta_original).strip()\n",
        "\n",
        "        # Passar a resposta original para o pipeline para humanização\n",
        "        resposta_humanizada = pipeline.invoke({\"query\": resposta_original})\n",
        "\n",
        "        # Garantir que a resposta reformulada seja uma string\n",
        "        if isinstance(resposta_humanizada, dict):\n",
        "            resposta_humanizada = resposta_humanizada.get(\"output\", \"\") or resposta_humanizada.get(\"query\", \"\")\n",
        "        resposta_humanizada = str(resposta_humanizada.content).strip()\n",
        "\n",
        "        return resposta_humanizada\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Desculpe, ocorreu um erro ao reformular a resposta: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRt_xvu1IHEX",
        "outputId": "24f7e7d1-bc90-4187-aaa5-39e0e007d51a"
      },
      "outputs": [],
      "source": [
        "print(df_diarias['SERVIDOR'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "YR1HwG-c-1VC",
        "outputId": "9bd8e550-8f99-4986-a0d6-b6c4c929fe11"
      },
      "outputs": [],
      "source": [
        "query = \"Pode me informar a quantidade de servidores diferentes?\"\n",
        "response = pandas_agent.invoke(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ8pS1a0H-NZ"
      },
      "source": [
        "#### Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CjV2uQiEztg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVXl64FzScVh"
      },
      "source": [
        "### RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4mPMserJQpQ",
        "outputId": "1557bf25-eae8-4e6e-b47d-8f83efc08fe9"
      },
      "outputs": [],
      "source": [
        "!pip install langchain faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MIs0ANXTWGo"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path=\"/content/servidor_ativo.csv\", csv_args={\"delimiter\": \";\"})\n",
        "documentos = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE3M6I2ZTkv6"
      },
      "outputs": [],
      "source": [
        "def reformat_document(doc):\n",
        "    # Divide o conteúdo em linhas\n",
        "    linhas = doc.page_content.split(\"\\n\")\n",
        "    info = {}\n",
        "    # Processa cada linha, extraindo chave e valor\n",
        "    for linha in linhas:\n",
        "        if \":\" in linha:\n",
        "            chave, valor = linha.split(\":\", 1)\n",
        "            # Se o valor estiver vazio após remover espaços, atribua \"não informado\"\n",
        "            valor_formatado = valor.strip() if valor.strip() else \"não informado\"\n",
        "            info[chave.strip()] = valor_formatado\n",
        "    # Cria um texto formatado de forma natural\n",
        "    texto_formatado = (\n",
        "        f\"{info.get('nome', 'não informado')}, matrícula {info.get('matricula', 'não informado')}, \"\n",
        "        f\"lotado em {info.get('lotacao', 'não informado')}, nível {info.get('nivel', 'não informado')}, \"\n",
        "        f\"cargo {info.get('cargo', 'não informado')}, área {info.get('area', 'não informado')}, \"\n",
        "        f\"situação {info.get('situacao', 'não informado')}, especialista em {info.get('especialidade', 'não informado')} ingresso em {info.get('ingresso', 'não informado')}.\"\n",
        "    )\n",
        "    # Atualiza o conteúdo do documento\n",
        "    #doc.page_content = texto_formatado\n",
        "    return texto_formatado\n",
        "\n",
        "# Aplica a reformatação a todos os documentos\n",
        "#documentos_formatados = [reformat_document(doc) for doc in documentos]\n",
        "documento = [reformat_document(doc) for doc in documentos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ_xYjx6NnAx",
        "outputId": "94937dfb-654a-4991-cd77-6143982e00f1"
      },
      "outputs": [],
      "source": [
        "print(documento[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHTYHpXqSf7N",
        "outputId": "2ece78d2-6882-4569-94c4-83377f59f813"
      },
      "outputs": [],
      "source": [
        "# Definir o número de linhas por chunk\n",
        "num_linhas_por_chunk = 5\n",
        "\n",
        "# Dividir o documento único em linhas\n",
        "linhas = documento.split(\"\\n\")\n",
        "\n",
        "# Criar chunks de 50 linhas cada\n",
        "chunks = [\"\\n\".join(linhas[i : i + num_linhas_por_chunk]) for i in range(0, len(linhas), num_linhas_por_chunk)]\n",
        "\n",
        "# Exibir informações sobre os chunks gerados\n",
        "print(f\"Total de chunks gerados: {len(chunks)}\")\n",
        "print(\"\\n--- Exemplo de um Chunk ---\\n\")\n",
        "print(chunks[0])  # Exibir o primeiro chunk para conferência\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcx00cuVg50N"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API key here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agCawhypN61b"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KItXLodnN8x5"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "documentos = [Document(page_content=chunk) for chunk in chunks]\n",
        "vectorstore = FAISS.from_documents(documentos, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UgFKM41OBKW"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "chain = load_qa_chain(gemini, chain_type=\"stuff\")\n",
        "qa_chain = RetrievalQA(combine_documents_chain=chain, retriever=retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Rcb6ts8fd1b",
        "outputId": "54ac53d2-b740-4c6c-f98c-07abb83f0704"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retriever.invoke(\"Quantos servidores são especialistas em Operação de Computadores?\")\n",
        "for doc in retrieved_docs:\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk2F9ukyOCod",
        "outputId": "8b2a5754-beff-48aa-f5a0-a1eb7fc6b19f"
      },
      "outputs": [],
      "source": [
        "pergunta = \"Quantos servidores são especialistas em Operação de Computadores?\"\n",
        "resposta = qa_chain.run(pergunta)\n",
        "print(resposta)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GthkaiKwUh0Z",
        "nhQgIRVOShf7",
        "OQ8pS1a0H-NZ",
        "NVXl64FzScVh"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
